{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01923e3a-7673-4292-b09f-59b3c5ee810b",
   "metadata": {},
   "source": [
    "# Answer1\n",
    "Gradient Boosting Regression is a machine learning technique used for regression problems, where the goal is to predict a continuous numerical value. It is an ensemble learning method that combines the predictions of multiple weak learners (usually decision trees) to create a strong predictive model.\n",
    "\n",
    "The \"gradient\" in Gradient Boosting refers to the optimization technique used to minimize the loss function during the training process. The algorithm builds trees sequentially, with each tree attempting to correct the errors of the combined model built so far. The process involves minimizing the residual errors by fitting a new tree to the residual values of the previous predictions.\n",
    "\n",
    "Here's a brief overview of how Gradient Boosting Regression works:\n",
    "\n",
    "1. **Initialization:** The initial model is often a simple one, like the mean or median of the target variable.\n",
    "\n",
    "2. **Iteration (Boosting):** For each iteration, a weak learner (usually a shallow decision tree) is fit to the negative gradient (residuals) of the loss function with respect to the previous model's predictions.\n",
    "\n",
    "3. **Weighted Combination:** The predictions of each weak learner are then combined, and the process is repeated for a specified number of iterations or until a certain level of performance is achieved.\n",
    "\n",
    "4. **Regularization:** To prevent overfitting, regularization techniques, such as tree depth limitations or shrinkage, are often applied.\n",
    "\n",
    "Gradient Boosting Regression has become popular due to its high predictive accuracy and robustness against overfitting. Common implementations include XGBoost, LightGBM, and AdaBoost. These algorithms have various optimizations and enhancements to improve training speed and model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa749a42-305c-402f-b7f9-4730ad2869d6",
   "metadata": {},
   "source": [
    "# Answer2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc420b13-8376-416e-8057-b4f562e6d8dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 208.06\n",
      "R-squared: -1.50\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Generate a synthetic dataset for regression\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 3 * X.squeeze() + np.random.randn(100) * 2\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Gradient Boosting Regression from scratch\n",
    "class GradientBoostingRegressor:\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initial prediction is the mean of the target variable\n",
    "        initial_prediction = np.mean(y)\n",
    "        predictions = np.full_like(y, initial_prediction)\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Calculate the negative gradient (residuals)\n",
    "            residuals = y - predictions\n",
    "\n",
    "            # Fit a decision tree to the residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=3)\n",
    "            tree.fit(X, residuals)\n",
    "\n",
    "            # Make predictions with the new tree\n",
    "            tree_predictions = tree.predict(X)\n",
    "\n",
    "            # Update the combined predictions with a fraction (learning_rate) of the new predictions\n",
    "            predictions += self.learning_rate * tree_predictions\n",
    "\n",
    "            # Save the trained tree to the list of models\n",
    "            self.models.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Make predictions using all the trained trees\n",
    "        predictions = np.sum(self.learning_rate * model.predict(X) for model in self.models)\n",
    "        return predictions\n",
    "\n",
    "# Instantiate and train the Gradient Boosting Regressor\n",
    "gb_regressor = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1)\n",
    "gb_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = gb_regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.2f}\")\n",
    "print(f\"R-squared: {r2:.2f}\")\n",
    "\n",
    "# Plot the original data and the predictions\n",
    "plt.scatter(X_test, y_test, label='Actual')\n",
    "plt.scatter(X_test, y_pred, label='Predicted')\n",
    "plt.legend()\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('y')\n",
    "plt.title('Gradient Boosting Regression from Scratch')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a53c364-2afd-48d0-8aca-e74c5bb0d51b",
   "metadata": {},
   "source": [
    "# Answer3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00580186-0171-4e08-ba30-017c26f7bbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'learning_rate': 0.1, 'max_depth': 3, 'n_estimators': 50}\n",
      "Best Model Mean Squared Error: 206.35\n",
      "Best Model R-squared: -1.48\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "\n",
    "# Generate a synthetic dataset for regression\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1) * 10\n",
    "y = 3 * X.squeeze() + np.random.randn(100) * 2\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Updated GradientBoostingRegressor class\n",
    "class GradientBoostingRegressor(BaseEstimator, RegressorMixin):\n",
    "    def __init__(self, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.models = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        # Initial prediction is the mean of the target variable\n",
    "        initial_prediction = np.mean(y)\n",
    "        predictions = np.full_like(y, initial_prediction)\n",
    "\n",
    "        for _ in range(self.n_estimators):\n",
    "            # Calculate the negative gradient (residuals)\n",
    "            residuals = y - predictions\n",
    "\n",
    "            # Fit a decision tree to the residuals\n",
    "            tree = DecisionTreeRegressor(max_depth=self.max_depth)\n",
    "            tree.fit(X, residuals)\n",
    "\n",
    "            # Make predictions with the new tree\n",
    "            tree_predictions = tree.predict(X)\n",
    "\n",
    "            # Update the combined predictions with a fraction (learning_rate) of the new predictions\n",
    "            predictions += self.learning_rate * tree_predictions\n",
    "\n",
    "            # Save the trained tree to the list of models\n",
    "            self.models.append(tree)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Make predictions using all the trained trees\n",
    "        predictions = np.sum(self.learning_rate * model.predict(X) for model in self.models)\n",
    "        return predictions\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        return {'n_estimators': self.n_estimators, 'learning_rate': self.learning_rate, 'max_depth': self.max_depth}\n",
    "\n",
    "# Define the parameter grid for grid search\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 150],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Create a GradientBoostingRegressor instance\n",
    "base_model = GradientBoostingRegressor()\n",
    "\n",
    "# Create a GridSearchCV object\n",
    "grid_search = GridSearchCV(base_model, param_grid, scoring='neg_mean_squared_error', cv=5)\n",
    "\n",
    "# Perform the grid search on the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters from the grid search\n",
    "best_params = grid_search.best_params_\n",
    "print(\"Best Hyperparameters:\", best_params)\n",
    "\n",
    "# Instantiate the model with the best hyperparameters and fit it to the entire training set\n",
    "best_gb_model = GradientBoostingRegressor(**best_params)\n",
    "best_gb_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_best = best_gb_model.predict(X_test)\n",
    "\n",
    "# Evaluate the model with the best hyperparameters\n",
    "mse_best = mean_squared_error(y_test, y_pred_best)\n",
    "r2_best = r2_score(y_test, y_pred_best)\n",
    "\n",
    "print(f\"Best Model Mean Squared Error: {mse_best:.2f}\")\n",
    "print(f\"Best Model R-squared: {r2_best:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc43704-a43b-4ad1-91d0-e654d66ad86d",
   "metadata": {},
   "source": [
    "# Answer4\n",
    "In the context of Gradient Boosting, a weak learner is a model that performs slightly better than random chance on a given task. It is often a simple and relatively low-complexity model. The concept of weak learners is crucial to the success of Gradient Boosting, as the algorithm aims to combine multiple weak learners to create a strong predictive model.\n",
    "\n",
    "The term \"weak\" does not imply that the individual model is inherently poor but rather that it is not highly expressive or complex. Typically, decision trees with limited depth are used as weak learners in the case of Gradient Boosting for regression or classification tasks.\n",
    "\n",
    "Here are some characteristics of weak learners in the context of Gradient Boosting:\n",
    "\n",
    "1. **Low Complexity:** Weak learners are usually simple models with restricted complexity. In the case of decision trees, they might be shallow trees with a limited number of nodes.\n",
    "\n",
    "2. **Slightly Better than Random:** A weak learner should perform slightly better than random guessing on the task at hand. It should have some ability to capture patterns in the data.\n",
    "\n",
    "3. **Combinatorial Strength:** While individual weak learners might not be highly accurate, the strength of Gradient Boosting comes from the iterative combination of multiple weak learners. Each new weak learner is trained to correct the errors made by the ensemble of learners constructed so far.\n",
    "\n",
    "The idea behind using weak learners in Gradient Boosting is that by sequentially adding models to the ensemble, with each model focusing on the mistakes of the previous ones, the algorithm can gradually improve the overall performance and build a highly accurate predictive model.\n",
    "\n",
    "Commonly used weak learners in Gradient Boosting include shallow decision trees, often referred to as \"stumps\" when they have very few nodes. These weak learners are computationally efficient and help prevent overfitting during the boosting process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2048ba-d29a-450e-bac1-7de6422e130b",
   "metadata": {},
   "source": [
    "# Answer5\n",
    "The intuition behind the Gradient Boosting algorithm can be understood through the following key concepts:\n",
    "\n",
    "1. **Ensemble Learning:**\n",
    "   - Gradient Boosting is an ensemble learning technique that combines the predictions of multiple weak learners to create a strong, accurate predictive model.\n",
    "\n",
    "2. **Sequential Model Building:**\n",
    "   - The algorithm builds the ensemble sequentially, with each new model aiming to correct the errors of the combined ensemble built so far.\n",
    "\n",
    "3. **Gradient Descent Optimization:**\n",
    "   - The term \"Gradient\" in Gradient Boosting comes from the optimization technique used. It minimizes the loss function by adjusting the predictions of the model in the direction that reduces the error.\n",
    "\n",
    "4. **Residuals and Pseudo-Residuals:**\n",
    "   - At each iteration, the algorithm calculates the difference between the actual and predicted values (residuals). The new weak learner is then trained to predict these residuals (pseudo-residuals) rather than the actual target values.\n",
    "\n",
    "5. **Learning from Mistakes:**\n",
    "   - The idea is to sequentially add models to the ensemble, with each model focusing on the mistakes made by the combined ensemble constructed so far. This process is akin to learning from the errors of the previous models.\n",
    "\n",
    "6. **Combining Weak Learners:**\n",
    "   - Weak learners, often shallow decision trees, are used as base models. While individually weak, when combined, they contribute to a strong predictive model. Each new tree is fitted to the negative gradient of the loss function with respect to the current ensemble's predictions.\n",
    "\n",
    "7. **Regularization:**\n",
    "   - To prevent overfitting, regularization techniques such as tree depth limitation and shrinkage (learning rate) are often employed. These techniques help control the complexity of the final model.\n",
    "\n",
    "8. **Boosting for Weighted Corrections:**\n",
    "   - The term \"Boosting\" indicates the iterative nature of the algorithm, where each new weak learner is trained to give more weight to the instances that were misclassified or had higher residuals in the previous iterations.\n",
    "\n",
    "In summary, the intuition behind Gradient Boosting lies in building a strong predictive model by iteratively improving upon the weaknesses of the existing ensemble. It optimizes the model's predictions in the direction that minimizes the error, leading to a powerful and accurate final model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913d4578-220a-47e5-8dd2-e24a3b0bd0fa",
   "metadata": {},
   "source": [
    "# Answer6\n",
    "The Gradient Boosting algorithm builds an ensemble of weak learners sequentially. The process can be summarized in the following steps:\n",
    "\n",
    "1. **Initialization:**\n",
    "   - The process starts with an initial prediction, which is often the mean (or another simple statistic) of the target variable.\n",
    "\n",
    "2. **Compute Residuals:**\n",
    "   - Calculate the residuals by subtracting the initial prediction from the actual target values. These residuals represent the errors made by the current ensemble.\n",
    "\n",
    "3. **Train a Weak Learner:**\n",
    "   - Fit a weak learner (typically a shallow decision tree) to the residuals. The weak learner is trained to predict the negative gradient of the loss function with respect to the current ensemble's predictions. In other words, it focuses on correcting the errors made by the existing ensemble.\n",
    "\n",
    "4. **Update Predictions:**\n",
    "   - Update the ensemble's predictions by adding a fraction (learning rate) of the predictions from the newly trained weak learner. This step is a form of regularization, controlling the contribution of each weak learner to the overall ensemble.\n",
    "\n",
    "5. **Repeat:**\n",
    "   - Repeat steps 2-4 for a predefined number of iterations (number of trees or weak learners). In each iteration, a new weak learner is trained to correct the errors of the combined ensemble built so far.\n",
    "\n",
    "6. **Final Ensemble:**\n",
    "   - The final prediction is the sum of the predictions from all the weak learners in the ensemble. Each weak learner contributes a weighted amount to the final prediction based on its learning rate.\n",
    "\n",
    "The iterative nature of Gradient Boosting, where each new weak learner corrects the errors made by the existing ensemble, leads to the creation of a strong predictive model. The process is guided by the optimization of a loss function, which is chosen based on the type of problem (e.g., mean squared error for regression, log loss for classification).\n",
    "\n",
    "Key points to note:\n",
    "\n",
    "- The weak learners are typically shallow decision trees, often referred to as \"stumps.\"\n",
    "- The learning rate controls the contribution of each weak learner, acting as a regularization parameter.\n",
    "- The process continues until a predefined number of iterations is reached or until a certain level of performance is achieved.\n",
    "\n",
    "Gradient Boosting is known for its effectiveness and has been widely used in practice, often outperforming other machine learning algorithms in various scenarios. Popular implementations include XGBoost, LightGBM, and scikit-learn's GradientBoostingRegressor/GradientBoostingClassifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41588ccb-bccd-4f21-849f-33ad56fb3cc9",
   "metadata": {},
   "source": [
    "# Answer7\n",
    "Constructing the mathematical intuition of the Gradient Boosting algorithm involves understanding the optimization process, the use of weak learners, and the idea of building an additive model to minimize a loss function. Here are the key steps involved in developing the mathematical intuition of Gradient Boosting:\n",
    "\n",
    "1. **Initialize with a Simple Model:**\n",
    "   - Start with an initial prediction, often the mean (or another simple statistic) of the target variable. This serves as the baseline prediction.\n",
    "\n",
    "2. **Compute Residuals:**\n",
    "   - Calculate the residuals by subtracting the current prediction from the actual target values. The residuals represent the errors made by the current model.\n",
    "\n",
    "3. **Train a Weak Learner:**\n",
    "   - Fit a weak learner (usually a decision tree with limited depth) to the residuals. The weak learner aims to capture the patterns or structure in the residuals.\n",
    "\n",
    "4. **Compute Negative Gradient:**\n",
    "   - Compute the negative gradient of the loss function with respect to the current predictions. The negative gradient represents the direction in which the predictions should be adjusted to minimize the loss.\n",
    "\n",
    "5. **Update Predictions:**\n",
    "   - Update the current predictions by adding a fraction (learning rate) of the predictions from the weak learner. This step is a form of regularization, controlling the contribution of each weak learner to the overall model.\n",
    "\n",
    "6. **Repeat Steps 2-5:**\n",
    "   - Repeat the process by computing new residuals, training a new weak learner, and updating predictions. Each iteration focuses on correcting the errors made by the combined ensemble built so far.\n",
    "\n",
    "7. **Combine Weak Learners:**\n",
    "   - The final prediction is the sum of the predictions from all the weak learners in the ensemble. Each weak learner contributes to the final prediction based on its learning rate and the direction it provides to minimize the loss.\n",
    "\n",
    "8. **Optimize Loss Function:**\n",
    "   - The objective is to minimize a chosen loss function, such as mean squared error for regression or log loss for classification. The algorithm optimizes the model's parameters (tree structure, weights, etc.) in the direction that reduces the loss.\n",
    "\n",
    "9. **Regularization:**\n",
    "   - The learning rate and the addition of weak learners act as regularization mechanisms. They prevent overfitting by controlling the influence of each weak learner and ensuring a gradual adjustment of predictions.\n",
    "\n",
    "10. **Final Ensemble:**\n",
    "    - The final model is an additive combination of weak learners. The ensemble captures the complex relationships in the data by sequentially refining predictions.\n",
    "\n",
    "The mathematical intuition involves concepts from optimization, statistics, and machine learning. Understanding how the algorithm minimizes the loss function, uses weak learners, and builds an ensemble helps in gaining a deeper appreciation of Gradient Boosting's power and versatility. Additionally, the regularization mechanisms contribute to the algorithm's ability to generalize well to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3cbe5a-16fe-4dec-9bb1-aa138ef73e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
