{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47badaf8-0cc9-451e-9fd7-67b40ddad186",
   "metadata": {},
   "source": [
    "# Answer1\n",
    "Boosting is a machine learning ensemble technique that aims to improve the predictive performance of a model by combining the strengths of multiple weak learners (usually simple models) to create a strong learner. The basic idea behind boosting is to sequentially train a series of weak models, giving more emphasis to the examples that the previous models misclassified. This way, the subsequent models focus on the harder-to-predict instances, gradually improving overall performance.\n",
    "\n",
    "Here's a general overview of how boosting works:\n",
    "\n",
    "1. **Initialize weights:** Assign equal weights to all training examples.\n",
    "\n",
    "2. **Train a weak learner:** Fit a weak model (e.g., a decision tree) on the training data, giving more importance to the misclassified examples from the previous iteration.\n",
    "\n",
    "3. **Compute error:** Assess the performance of the weak learner on the training set.\n",
    "\n",
    "4. **Compute learner weight:** Calculate the weight of the weak learner based on its performance. Better-performing models get higher weights.\n",
    "\n",
    "5. **Update example weights:** Increase the weights of misclassified examples, making them more influential in the next iteration.\n",
    "\n",
    "6. **Repeat:** Repeat steps 2-5 for a predefined number of iterations or until a certain level of performance is reached.\n",
    "\n",
    "7. **Combine weak learners:** Combine the weak learners, each weighted by its performance, to form a strong learner.\n",
    "\n",
    "Popular boosting algorithms include AdaBoost (Adaptive Boosting), Gradient Boosting, and XGBoost (Extreme Gradient Boosting). These algorithms have been successfully applied in various machine learning tasks, such as classification and regression, and are known for their ability to handle complex relationships in data and improve model accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eac3850-f2ca-48f9-94b3-bbfffef56b64",
   "metadata": {},
   "source": [
    "# Answer2\n",
    "**Advantages of Boosting Techniques:**\n",
    "\n",
    "1. **Improved Accuracy:** Boosting can significantly improve the predictive accuracy of models. By combining multiple weak learners, it focuses on correcting errors, leading to better overall performance.\n",
    "\n",
    "2. **Handles Complex Relationships:** Boosting algorithms, such as Gradient Boosting and XGBoost, are capable of capturing complex relationships in the data. They can handle non-linearities and interactions between features effectively.\n",
    "\n",
    "3. **Reduces Overfitting:** Boosting helps in reducing overfitting by emphasizing the correction of misclassified instances. The sequential nature of training ensures that the model pays more attention to challenging examples, which can contribute to a more generalized model.\n",
    "\n",
    "4. **Versatility:** Boosting techniques can be applied to various types of machine learning tasks, including classification, regression, and ranking.\n",
    "\n",
    "5. **Feature Importance:** Many boosting algorithms provide a measure of feature importance, helping users understand which features contribute more to the model's predictions.\n",
    "\n",
    "**Limitations of Boosting Techniques:**\n",
    "\n",
    "1. **Sensitivity to Noisy Data:** Boosting algorithms can be sensitive to noisy data and outliers. If the dataset contains significant noise, the boosting process may focus too much on these outliers, leading to suboptimal performance.\n",
    "\n",
    "2. **Computational Complexity:** Training multiple weak learners sequentially can make boosting computationally expensive, especially if the weak learners are complex. However, some implementations, like XGBoost, have optimizations to mitigate this issue.\n",
    "\n",
    "3. **Requires Tuning:** Boosting algorithms often have several hyperparameters that need to be tuned for optimal performance. This tuning process can be time-consuming and may require domain expertise.\n",
    "\n",
    "4. **Limited Interpretability:** The ensemble nature of boosting models can make them less interpretable compared to individual models. Understanding the contribution of each weak learner can be challenging.\n",
    "\n",
    "5. **Potential for Overfitting:** While boosting can help reduce overfitting, there is still a risk, especially if the number of weak learners is too high or if the model is excessively complex.\n",
    "\n",
    "In summary, boosting techniques offer significant advantages in terms of accuracy and handling complex relationships but require careful tuning and may be sensitive to certain types of data. It's essential to understand the characteristics of the dataset and the problem at hand when deciding whether to use boosting or another machine learning approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce306076-0ca5-47a9-a466-781594d11fe9",
   "metadata": {},
   "source": [
    "# Answer3\n",
    "Boosting is an ensemble learning technique that combines the predictions of multiple weak learners (often simple models) to create a strong learner with improved predictive performance. The key idea behind boosting is to sequentially train weak models, giving more emphasis to the examples that the previous models misclassified. This allows the ensemble to focus on the instances that are more challenging and gradually improve overall predictive accuracy. Here's a step-by-step explanation of how boosting works:\n",
    "\n",
    "1. **Initialize Weights:** Assign equal weights to all training examples. Each example is initially given equal importance.\n",
    "\n",
    "2. **Train a Weak Learner:** Fit a weak model (e.g., a decision tree) on the training data. The weak learner's task is to make predictions, but its simplicity ensures that it may not perform well on its own.\n",
    "\n",
    "3. **Compute Error:** Assess the performance of the weak learner on the training set. Identify the instances that the model misclassified.\n",
    "\n",
    "4. **Compute Learner Weight:** Calculate the weight of the weak learner based on its performance. A better-performing model will be given a higher weight.\n",
    "\n",
    "5. **Update Example Weights:** Increase the weights of the misclassified examples. This makes them more influential in the next iteration, forcing the subsequent weak learners to focus more on the challenging instances.\n",
    "\n",
    "6. **Repeat:** Steps 2-5 are repeated for a predefined number of iterations or until a certain level of performance is reached. In each iteration, a new weak learner is trained with adjusted weights.\n",
    "\n",
    "7. **Combine Weak Learners:** Combine all the weak learners, each weighted by its performance. The final prediction is made by taking a weighted sum (or a vote) of the weak learners' predictions.\n",
    "\n",
    "The boosting process aims to iteratively correct the errors made by the previous weak learners, leading to a strong ensemble model that can adapt well to the complexities of the data. Common boosting algorithms include AdaBoost, Gradient Boosting, and XGBoost, each with variations in the weighting and updating strategies. The boosting technique has been successful in various machine learning applications, including classification and regression tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c818cabd-d289-4f97-9544-0e5ef9b49723",
   "metadata": {},
   "source": [
    "# Answer4\n",
    "There are several types of boosting algorithms, each with its own characteristics and variations. Here are some of the prominent boosting algorithms:\n",
    "\n",
    "1. **AdaBoost (Adaptive Boosting):** AdaBoost is one of the earliest and most well-known boosting algorithms. It assigns weights to the training examples and adjusts these weights iteratively to emphasize misclassified instances, allowing subsequent weak learners to focus on correcting errors.\n",
    "\n",
    "2. **Gradient Boosting (GBM):** Gradient Boosting builds a series of weak learners sequentially, where each new model corrects the errors of the previous ones. It optimizes a cost function by using gradient descent at each step. Popular implementations include scikit-learn's `GradientBoostingClassifier` and `GradientBoostingRegressor`.\n",
    "\n",
    "3. **XGBoost (Extreme Gradient Boosting):** XGBoost is an optimized and efficient version of gradient boosting. It incorporates regularization techniques, parallel processing, and tree-pruning methods to enhance speed and performance. XGBoost is widely used in data science competitions and real-world applications.\n",
    "\n",
    "4. **LightGBM:** LightGBM is a gradient boosting framework developed by Microsoft that focuses on speed and efficiency. It uses a histogram-based learning method for faster training and is particularly suitable for large datasets.\n",
    "\n",
    "5. **CatBoost:** CatBoost is a boosting algorithm developed by Yandex that is designed to handle categorical features efficiently. It employs techniques to reduce the need for extensive preprocessing of categorical variables.\n",
    "\n",
    "6. **LogitBoost:** LogitBoost is a boosting algorithm specifically designed for binary classification problems. It optimizes the logistic loss function and adapts the weights of the training examples.\n",
    "\n",
    "7. **BrownBoost:** BrownBoost is a boosting algorithm that minimizes the exponential loss function. It introduces a parameter to control the trade-off between training error and model complexity.\n",
    "\n",
    "8. **LPBoost (Linear Programming Boosting):** LPBoost is a boosting algorithm that minimizes the classification error subject to linear constraints. It is suitable for scenarios where incorporating prior knowledge through linear constraints is beneficial.\n",
    "\n",
    "These algorithms vary in their optimization techniques, handling of categorical features, and approaches to regularization. The choice of which algorithm to use often depends on the specific characteristics of the data and the problem at hand. XGBoost, LightGBM, and CatBoost are particularly popular due to their efficiency and performance improvements over traditional boosting methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48d7fbd-6962-4ae1-bcd5-b08047d07471",
   "metadata": {},
   "source": [
    "# Answer5\n",
    "Boosting algorithms typically have a variety of parameters that can be tuned to optimize performance and control the behavior of the model during training. Here are some common parameters found in boosting algorithms:\n",
    "\n",
    "1. **Number of Estimators (n_estimators):** This parameter specifies the number of weak learners (e.g., decision trees) to be used in the ensemble. Increasing the number of estimators can improve model performance but may also increase training time and risk overfitting.\n",
    "\n",
    "2. **Learning Rate (or Step Size):** The learning rate controls the contribution of each weak learner to the final ensemble. A smaller learning rate requires more weak learners to achieve similar performance but can improve generalization. It is typically set between 0 and 1.\n",
    "\n",
    "3. **Max Depth (max_depth):** The maximum depth of each weak learner (e.g., decision tree). Deeper trees can capture more complex relationships in the data but may lead to overfitting.\n",
    "\n",
    "4. **Subsample:** This parameter controls the fraction of training data to be used for fitting each weak learner. Setting it to less than 1.0 can introduce randomness and reduce overfitting.\n",
    "\n",
    "5. **Loss Function:** Boosting algorithms often support different loss functions, such as exponential loss, logistic loss, or squared loss, depending on the nature of the problem (e.g., classification or regression).\n",
    "\n",
    "6. **Regularization Parameters:** Boosting algorithms may include regularization parameters to prevent overfitting. These parameters include lambda (L2 regularization), alpha (L1 regularization), and gamma (minimum loss reduction required to make a further partition).\n",
    "\n",
    "7. **Feature Importance:** Many boosting algorithms provide a measure of feature importance, which indicates the contribution of each feature to the model's predictions. These importance scores can be used for feature selection or interpretation.\n",
    "\n",
    "8. **Early Stopping:** Early stopping is a technique to prevent overfitting by stopping the training process when the performance on a validation set stops improving.\n",
    "\n",
    "9. **Tree-specific Parameters:** Some boosting algorithms, like XGBoost and LightGBM, offer additional parameters specific to the weak learner (e.g., tree booster parameters in XGBoost).\n",
    "\n",
    "10. **Categorical Features Handling:** Parameters related to handling categorical features efficiently, such as CatBoost's `cat_features` parameter.\n",
    "\n",
    "11. **Objective Function:** The objective function defines the metric to be optimized during training, such as log loss for classification or mean squared error for regression.\n",
    "\n",
    "These parameters may vary slightly depending on the specific boosting algorithm you're using. It's essential to understand the effect of each parameter and perform hyperparameter tuning to find the best combination for your particular dataset and problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2d4bd64-60ca-4390-b318-aefd5dcb4169",
   "metadata": {},
   "source": [
    "# Answer6\n",
    "Boosting algorithms combine weak learners to create a strong learner through a weighted sum (or voting) of their individual predictions. The combination process involves assigning different weights to each weak learner based on its performance during training. The overall prediction is then determined by considering the weighted contributions of all the weak learners. Here's a general outline of how boosting algorithms combine weak learners:\n",
    "\n",
    "1. **Initialize Weights:** At the beginning of the boosting process, each training example is assigned an equal weight.\n",
    "\n",
    "2. **Train Weak Learner:** Fit a weak learner (e.g., a decision tree) on the training data. The weak learner makes predictions, but its simplicity ensures that it may not perform well on its own.\n",
    "\n",
    "3. **Compute Error:** Evaluate the performance of the weak learner on the training set. Identify the instances that the model misclassified.\n",
    "\n",
    "4. **Compute Learner Weight:** Calculate the weight of the weak learner based on its performance. A better-performing model will be given a higher weight. The weight is often determined by the error rate, with lower errors resulting in higher weights.\n",
    "\n",
    "5. **Update Example Weights:** Increase the weights of the misclassified examples. This makes them more influential in the next iteration, forcing subsequent weak learners to focus more on the challenging instances.\n",
    "\n",
    "6. **Repeat:** Steps 2-5 are repeated for a predefined number of iterations or until a certain level of performance is reached. In each iteration, a new weak learner is trained with adjusted weights.\n",
    "\n",
    "7. **Combine Weak Learners:** The final prediction is made by combining all the weak learners, each weighted by its performance. The weighted sum of individual predictions or a weighted voting scheme is used to produce the overall prediction.\n",
    "\n",
    "Mathematically, the final prediction (\\(F(x)\\)) is often expressed as follows:\n",
    "\n",
    "[ F(x) = \\sum_{t=1}^{T} \\alpha_t f_t(x) ]\n",
    "\n",
    "where:\n",
    "- \\( T \\) is the number of weak learners,\n",
    "- \\( \\alpha_t \\) is the weight assigned to the \\( t \\)-th weak learner,\n",
    "- \\( f_t(x) \\) is the prediction made by the \\( t \\)-th weak learner.\n",
    "\n",
    "This combination process ensures that the boosting algorithm gives more emphasis to the predictions of weak learners that perform well on challenging instances, effectively creating a strong learner capable of handling complex relationships in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79eeb8b4-7468-4666-8f04-26eaf0ee9cae",
   "metadata": {},
   "source": [
    "# Answer7\n",
    "AdaBoost, short for Adaptive Boosting, is an ensemble learning algorithm that belongs to the family of boosting algorithms. It was introduced by Yoav Freund and Robert Schapire in 1996. AdaBoost focuses on combining the predictions of multiple weak learners (typically simple models) to create a strong learner with improved predictive performance. The key idea behind AdaBoost is to sequentially train weak models, giving more emphasis to the examples that the previous models misclassified.\n",
    "\n",
    "Here's how AdaBoost works:\n",
    "\n",
    "1. **Initialize Weights:** Assign equal weights to all training examples. Each example is given equal importance.\n",
    "\n",
    "2. **Train Weak Learner:** Fit a weak learner (e.g., a decision tree) on the training data. The weak learner's task is to make predictions, but its simplicity ensures that it may not perform well on its own.\n",
    "\n",
    "3. **Compute Error:** Assess the performance of the weak learner on the training set. Identify the instances that the model misclassified.\n",
    "\n",
    "4. **Compute Learner Weight:** Calculate the weight of the weak learner based on its performance. A better-performing model will be given a higher weight. The weight is often determined by the error rate, with lower errors resulting in higher weights.\n",
    "\n",
    "5. **Update Example Weights:** Increase the weights of the misclassified examples. This makes them more influential in the next iteration, forcing subsequent weak learners to focus more on the challenging instances.\n",
    "\n",
    "6. **Repeat:** Steps 2-5 are repeated for a predefined number of iterations or until a certain level of performance is reached. In each iteration, a new weak learner is trained with adjusted weights.\n",
    "\n",
    "7. **Combine Weak Learners:** The final prediction is made by combining all the weak learners, each weighted by its performance. The weighted sum of individual predictions or a weighted voting scheme is used to produce the overall prediction.\n",
    "\n",
    "Mathematically, the final prediction (\\(F(x)\\)) is often expressed as follows:\n",
    "\n",
    "[ F(x) = \\sum_{t=1}^{T} \\alpha_t f_t(x) ]\n",
    "\n",
    "where:\n",
    "- \\( T \\) is the number of weak learners,\n",
    "- \\( \\alpha_t \\) is the weight assigned to the \\( t \\)-th weak learner,\n",
    "- \\( f_t(x) \\) is the prediction made by the \\( t \\)-th weak learner.\n",
    "\n",
    "This combination process ensures that the boosting algorithm gives more emphasis to the predictions of weak learners that perform well on challenging instances, effectively creating a strong learner capable of handling complex relationships in the data. AdaBoost has been widely used for binary classification problems and has proven to be effective in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "397a7ff6-2148-49cc-a24d-d4463642fdf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a weak learner (decision tree with max_depth=1)\n",
    "base_model = DecisionTreeClassifier(max_depth=1)\n",
    "\n",
    "# Create an AdaBoost Classifier with 50 weak learners\n",
    "adaboost_clf = AdaBoostClassifier(base_model, n_estimators=50, learning_rate=1.0, random_state=42)\n",
    "\n",
    "# Train the AdaBoost model\n",
    "adaboost_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = adaboost_clf.predict(X_test)\n",
    "\n",
    "# Evaluate the accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20390c5-ae1e-404a-aaf1-edca057f57fc",
   "metadata": {},
   "source": [
    "# Answer8\n",
    "The loss function used in the AdaBoost algorithm is the exponential loss function. Specifically, AdaBoost minimizes the exponential loss function to sequentially train weak learners and assign weights to each weak learner's contribution in the final prediction.\n",
    "\n",
    "The exponential loss function (\\(L(y, f(x))\\)) for binary classification is defined as:\n",
    "\n",
    "[ L(y, f(x)) = e^{-y \\cdot f(x)} ]\n",
    "\n",
    "where:\n",
    "- \\( y \\) is the true class label (\\(y = \\pm 1\\)),\n",
    "- \\( f(x) \\) is the prediction made by the weak learner.\n",
    "\n",
    "In the context of AdaBoost, \\(y\\) is either +1 or -1, indicating the positive or negative class, and \\(f(x)\\) is the weighted sum of predictions from all weak learners.\n",
    "\n",
    "During each iteration of AdaBoost, the algorithm assigns weights to training examples based on their misclassification. The weights are adjusted to give more emphasis to misclassified examples, effectively guiding subsequent weak learners to focus on correcting the errors made by the previous ones.\n",
    "\n",
    "The exponential loss function is chosen because it is a convex, smooth function that strongly penalizes misclassifications. Minimizing this loss function encourages the boosting algorithm to prioritize examples that are more difficult to classify correctly, leading to improved generalization and adaptability to complex datasets.\n",
    "\n",
    "In summary, AdaBoost minimizes the exponential loss function to train weak learners sequentially and combine their predictions to form a strong learner. The weighted combination ensures that more weight is given to weak learners that perform well on challenging instances, contributing to the overall effectiveness of the ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539e3249-536a-44dd-988d-e32e0c079250",
   "metadata": {},
   "source": [
    "# Answer9\n",
    "The AdaBoost algorithm updates the weights of misclassified samples during each iteration to give more emphasis to the examples that the current weak learner misclassifies. The objective is to guide subsequent weak learners to focus on the instances that are more challenging to classify correctly. The update process involves increasing the weights of misclassified samples and decreasing the weights of correctly classified samples. Here's how the weight update is typically performed:\n",
    "\n",
    "Let's denote:\n",
    "- \\(w_i\\) as the weight assigned to the \\(i\\)-th training example.\n",
    "- \\(D_t\\) as the set of weights at iteration \\(t\\).\n",
    "- \\(h_t(x_i)\\) as the prediction made by the \\(t\\)-th weak learner on the \\(i\\)-th example.\n",
    "\n",
    "1. **Compute Error:** Calculate the weighted error rate (\\(\\epsilon_t\\)) of the weak learner \\(h_t\\) on the training set:\n",
    "\n",
    "\n",
    "   where \\(N\\) is the number of training examples, \\(y_i\\) is the true label of the \\(i\\)-th example, and \\(\\mathbb{1}(\\cdot)\\) is the indicator function.\n",
    "\n",
    "2. **Compute Weak Learner Weight (\\(\\alpha_t\\)):** Calculate the weight (\\(\\alpha_t\\)) assigned to the \\(t\\)-th weak learner:\n",
    "\n",
    "\n",
    "   The weight \\(\\alpha_t\\) is proportional to the log-odds of the weak learner's performance.\n",
    "\n",
    "3. **Update Example Weights:** Update the weights of the training examples for the next iteration:\n",
    "\n",
    "\n",
    "   The weights of misclassified examples (\\(y_i \\cdot h_t(x_i) \\neq 1\\)) will increase, making them more influential in the next iteration. Correctly classified examples will have their weights decreased.\n",
    "\n",
    "4. **Normalize Weights:** Normalize the updated weights to ensure they sum to 1:\n",
    "\n",
    "   This normalization step ensures that the weights form a valid probability distribution.\n",
    "\n",
    "These steps are repeated for a predefined number of iterations or until a certain criterion is met. The combination of weak learners in AdaBoost is achieved by computing a weighted sum of their predictions during the final prediction step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09758cd6-9ebc-4eaf-83c1-06f4b11ad9d2",
   "metadata": {},
   "source": [
    "# Answer10\n",
    "Increasing the number of estimators (weak learners) in the AdaBoost algorithm can have both positive and negative effects on the model's performance. Here are the key effects of increasing the number of estimators:\n",
    "\n",
    "**Positive Effects:**\n",
    "\n",
    "1. **Improved Training Accuracy:** In general, increasing the number of estimators tends to improve the training accuracy. This is because each weak learner is trained to correct the errors made by the previous ones, and a larger ensemble has more capacity to capture complex relationships in the data.\n",
    "\n",
    "2. **Better Generalization:** AdaBoost is prone to overfitting when the number of estimators is too small. Increasing the number of estimators can mitigate overfitting and improve the model's ability to generalize to new, unseen data.\n",
    "\n",
    "3. **Reduced Variance:** A larger ensemble typically reduces the variance of the model, making it more stable and less sensitive to small changes in the training data.\n",
    "\n",
    "**Negative Effects:**\n",
    "\n",
    "1. **Increased Training Time:** Training more weak learners increases the computational cost. AdaBoost is an iterative algorithm, and each iteration involves training a weak learner, updating weights, and computing the final prediction. As the number of estimators grows, the training time also increases.\n",
    "\n",
    "2. **Potential for Overfitting:** While increasing the number of estimators can reduce overfitting, there is a point beyond which adding more weak learners may lead to overfitting on the training data, especially if the data contains noise or outliers.\n",
    "\n",
    "3. **Diminishing Returns:** The improvement in performance may diminish as the number of estimators becomes very large. At some point, the additional weak learners may not contribute significantly to the model's accuracy, and the computational cost may not be justified.\n",
    "\n",
    "It's essential to strike a balance and perform model evaluation using validation or test datasets to find the optimal number of estimators. Common practice involves monitoring the model's performance on a validation set and selecting the number of estimators that provides the best trade-off between training accuracy and generalization to new data. Cross-validation can also be useful for model selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01425ca7-3ec0-4f34-b080-a9f5ff75f738",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
