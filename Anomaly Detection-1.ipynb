{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f231a5d-a06d-42e4-aac7-4098e94834a9",
   "metadata": {},
   "source": [
    "## Q1. What is anomaly detection and what is its purpose?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e29b4093-b5ef-4d72-9569-121cc90c0128",
   "metadata": {},
   "source": [
    "Anomaly detection is a process in machine learning that identifies data points, events, and observations that deviate from a data set’s normal behavior. It is used to identify features, events, or conditions that deviate from the norm and might be clues to potentially harmful scenarios, including fraud, cyber attacks, medical issues, and structural or functional flaws. Anomaly detection can serve multiple purposes, such as removing outliers in a training set before fitting a machine learning model. \n",
    "\n",
    "There are three types of anomaly detection: supervised, unsupervised, and semi-supervised. Supervised anomaly detection requires a labeled dataset containing both normal and anomalous samples to construct a predictive model to classify future data points. Unsupervised anomaly detection does not require labeled data and is used to identify anomalies in data that do not conform to expected patterns. Semi-supervised anomaly detection relies on a small amount of labeled data to validate and select the best performing model trained on normal data (or data with no anomalies).\n",
    "\n",
    "Anomaly detection is used in various applications such as fraud and intrusion detection, health monitoring, financial transactions, and IT and cyber security. In machine learning, it is used to identify anomalies in data that may indicate system failures, security breaches, or potential opportunities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4fe32f2-97dc-4e54-a279-124b4f7ed3a2",
   "metadata": {},
   "source": [
    "## Q2. What are the key challenges in anomaly detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ad2ca48-82db-4ad1-a017-414beb3fd69b",
   "metadata": {},
   "source": [
    "Anomaly detection is a complex process that involves identifying rare items, events, or observations that deviate from normal behavior or patterns in data. Some of the key challenges in anomaly detection include:\n",
    "\n",
    "1. **Extracting useful features appropriately**: The quality of the features used in anomaly detection is critical to the accuracy of the results. Selecting the right features and extracting them appropriately is a challenge.\n",
    "2. **Defining what is considered \"normal\"**: Defining what is considered normal behavior or patterns in data is subjective and can vary depending on the context.\n",
    "3. **Dealing with the situations where there are significantly more normal values than anomalies**: In many cases, the number of normal values in a dataset is significantly higher than the number of anomalies. This can make it difficult to identify the anomalies.\n",
    "4. **Separating noise from real outliers**: Anomalies can be difficult to distinguish from noise or other outliers in the data.\n",
    "5. **Difficulties brought by high dimensionality and the enormous amount of data**: Anomaly detection can be challenging when dealing with high-dimensional data or large datasets. Traditional statistical methods may not be effective in such cases.\n",
    "\n",
    "These challenges can be addressed by using appropriate anomaly detection techniques and algorithms, such as supervised, unsupervised, and semi-supervised anomaly detection. Additionally, it is important to have a clear understanding of the data and the context in which it is being used to ensure that the right features are selected and the anomalies are identified accurately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e81c1e-1674-4fcf-afc8-01b38920f491",
   "metadata": {},
   "source": [
    "## Q3. How does unsupervised anomaly detection differ from supervised anomaly detection?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aceadd84-8013-474a-8d25-117acb64aec2",
   "metadata": {},
   "source": [
    "In supervised anomaly detection, a labeled dataset containing both normal and anomalous samples is used to construct a predictive model to classify future data points. In contrast, unsupervised anomaly detection does not require labeled data and is used to identify anomalies in data that do not conform to expected patterns. Unsupervised anomaly detection algorithms are based on pattern matching and use a general outlier-detection mechanism. \n",
    "\n",
    "Semi-supervised anomaly detection is another type of anomaly detection that relies on a small amount of labeled data to validate and select the best performing model trained on normal data (or data with no anomalies).\n",
    "\n",
    "In summary, the main difference between supervised and unsupervised anomaly detection is the method used. Supervised anomaly detection requires labeled data, while unsupervised anomaly detection does not.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9611d7e9-6178-446a-8e5a-573536e67d97",
   "metadata": {},
   "source": [
    "## Q4. What are the main categories of anomaly detection algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46479895-7260-4484-8cff-e6125aa46664",
   "metadata": {},
   "source": [
    "Anomaly detection algorithms can be categorized into several categories based on their working mechanisms. Here are some of the main categories of anomaly detection algorithms:\n",
    "\n",
    "1. **Statistical-based algorithms**: These algorithms use statistical methods to identify anomalies in data. They are based on the assumption that anomalies are rare events that can be detected by analyzing the statistical properties of the data.\n",
    "2. **Density-based algorithms**: These algorithms identify anomalies as areas of low density in the data. They are based on the assumption that anomalies are located in regions of the data that have a low probability density.\n",
    "3. **Distance-based algorithms**: These algorithms identify anomalies as data points that are far from the majority of the data points. They are based on the assumption that anomalies are located far from the normal data points.\n",
    "4. **Clustering-based algorithms**: These algorithms identify anomalies as data points that do not belong to any cluster or belong to a small cluster. They are based on the assumption that anomalies are located in regions of the data that are not well-clustered.\n",
    "5. **Isolation-based algorithms**: These algorithms identify anomalies as data points that are isolated from the rest of the data. They are based on the assumption that anomalies are located far from the normal data points and can be isolated using a boundary.\n",
    "6. **Ensemble-based algorithms**: These algorithms combine multiple anomaly detection algorithms to improve the accuracy of the results. They are based on the assumption that different algorithms may perform better on different types of data.\n",
    "7. **Subspace-based algorithms**: These algorithms identify anomalies in subspaces of the data. They are based on the assumption that anomalies may only occur in certain subspaces of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed42ffc5-a425-4996-9976-3c8ace3410d9",
   "metadata": {},
   "source": [
    "## Q5. What are the main assumptions made by distance-based anomaly detection methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58ca835-9d0d-4f1d-bd97-da8ecc10a402",
   "metadata": {},
   "source": [
    "Distance-based anomaly detection methods rely on the assumption that normal data points are close to their neighbors, while anomalous data points are far from the normal data. These methods use a distance from a considered test point to its nearest neighbors to classify points with less than p neighboring points as anomalous or outliers. The key assumption underlying nearest neighbor-based anomaly detection methods is that normal points lie in dense neighborhoods and anomalous points lie in sparse neighborhoods. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3a8cd11-907f-4ce5-9d86-9b1b618f66f6",
   "metadata": {},
   "source": [
    "## Q6. How does the LOF algorithm compute anomaly scores?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c204e0d-1006-4d02-aa77-1c851a94362b",
   "metadata": {},
   "source": [
    "The Local Outlier Factor (LOF) algorithm is an unsupervised anomaly detection method that computes the local density deviation of a given data point with respect to its neighbors. It considers as outliers the samples that have a substantially lower density than their neighbors. The LOF algorithm computes an anomaly score by using the local density of each sample point with respect to the points in its surrounding neighborhood. The local density is inversely correlated with the average distance from a point to its nearest neighbors. Anomaly score values greater than 1.0 usually indicate the anomaly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b342bb3-bb91-41db-84c4-deef62cb6459",
   "metadata": {},
   "source": [
    "## Q7. What are the key parameters of the Isolation Forest algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a844a557-34e5-400a-8e07-9e723ce1d3f0",
   "metadata": {},
   "source": [
    "The Isolation Forest algorithm is an unsupervised anomaly detection method that uses a random forest to isolate anomalies. The key parameters of the Isolation Forest algorithm are:\n",
    "\n",
    "1. **Number of trees / estimators**: This parameter determines the size of the forest.\n",
    "2. **Contamination**: This parameter specifies the fraction of the dataset that contains abnormal instances.\n",
    "3. **Max samples**: This parameter determines the number of samples to draw from the training set to train each Isolation Tree with.\n",
    "4. **Max depth**: This parameter determines how deep the tree should be, which can be used to trim the tree and make things faster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061f312a-2289-477f-90c0-e15ff16ed036",
   "metadata": {},
   "source": [
    "## Q8. If a data point has only 2 neighbours of the same class within a radius of 0.5, what is its anomaly score using KNN with K=10?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cb7f5315-543f-4469-8e97-5769ab968fca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>NearestNeighbors(n_neighbors=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NearestNeighbors</label><div class=\"sk-toggleable__content\"><pre>NearestNeighbors(n_neighbors=10)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "NearestNeighbors(n_neighbors=10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# Data point\n",
    "data_point = np.array([[1, 2]])  # taking a exaple data\n",
    "\n",
    "# Simulated data (example dataset)\n",
    "data = np.array([[1, 1], [12, 2], [3, 3], [4, 2], [5, 53], [6, 6],[7,7],[8,8],[9,19],[111,10]])\n",
    "\n",
    "# Create a KNN model\n",
    "k = 10\n",
    "knn_model = NearestNeighbors(n_neighbors=k)\n",
    "knn_model.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c4e862c-ed2b-4fc8-b362-c18644bccaf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly Score: 1.618033988749895\n"
     ]
    }
   ],
   "source": [
    "# Find the indices and distances of the K nearest neighbors\n",
    "distances, indices = knn_model.kneighbors(data_point)\n",
    "\n",
    "# Assuming indices of neighbors of the same class within radius 0.5\n",
    "neighbors_within_radius = 2  # Number of neighbors within radius 0.5\n",
    "\n",
    "# Calculate anomaly score\n",
    "anomaly_score = np.sum(distances[0][:neighbors_within_radius]) / neighbors_within_radius\n",
    "\n",
    "print(f\"Anomaly Score: {anomaly_score}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb05f5a2-9c32-43af-b276-3244dcb14889",
   "metadata": {},
   "source": [
    "## Q9. Using the Isolation Forest algorithm with 100 trees and a dataset of 3000 data points, what is the anomaly score for a data point that has an average path length of 5.0 compared to the average path length of the trees?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "41a470f7-6f6f-4651-833b-75c7691690f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anomaly Score: 0.7957242830757882\n"
     ]
    }
   ],
   "source": [
    "# Given data\n",
    "num_trees = 100\n",
    "total_data_points = 3000\n",
    "data_point_average_path_length = 5.0\n",
    "\n",
    "# Calculate the average path length of the trees in Isolation Forest\n",
    "average_path_length_trees = 2 * (np.log(total_data_points - 1) + 0.5772156649) - 2 * (total_data_points - 1) / total_data_points\n",
    "\n",
    "# Calculate the anomaly score for the data point\n",
    "anomaly_score = 2 ** (-data_point_average_path_length / average_path_length_trees)\n",
    "\n",
    "print(f\"Anomaly Score: {anomaly_score}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
